{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Работаем с извлечением данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем библиотеку для работы с Postgres на питоне\n",
    "import psycopg2\n",
    "\n",
    "# Пытаемся подключиться к БД, при ошибке выскакивает alert\n",
    "try:\n",
    "    # Создаем подключение к БД\n",
    "    conn = psycopg2.connect(dbname='postgres', user='postgres', password='123', host='localhost')\n",
    "    # Создаем объект курсора для подключенной БД\n",
    "    cursor = conn.cursor()\n",
    "except:\n",
    "    print('Не удалось подключиться к БД!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запросом извлекаем данные из БД\n",
    "cursor.execute('SELECT name, description, salary, region_name FROM jobs.joblist')\n",
    "# Сохраняем и предобрабатываем данные\n",
    "all_info = list(map(list, cursor.fetchall()))\n",
    "# Запросом извлекаем данные из БД\n",
    "cursor.execute('SELECT main_skills FROM jobs.joblist')\n",
    "# Сохраняем и предобрабатываем данные\n",
    "all_skills = list(set(sum([i[0].split(';') for i in list(map(list, cursor.fetchall()))], [])))\n",
    "# Удаляем лишний элемент из массива\n",
    "del all_skills[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Закрываем объект курсора\n",
    "cursor.close()\n",
    "# Закрываем соединение с БД\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Работаем с преодбработкой данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортиурем библиотеку для леметизации слов\n",
    "import spacy\n",
    "# Импортируем библиотеку для работы со строками\n",
    "import re\n",
    "\n",
    "# Загружаем предобученную модель для леметизации русских слов\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "\n",
    "# Циклом проходимся по всем описаниям\n",
    "for i in range(len(all_info)):\n",
    "    # Сохраняем описание и удаляем из него лишнии знаки\n",
    "    description = re.sub(r'[—\\'\"(){}[\\]/.,?!-_:;]', r'', all_info[i][1])\n",
    "    # Получаем лемитизированные описание\n",
    "    doc = nlp(description)\n",
    "    # Заменяем старое описание на новое предобработаннео\n",
    "    all_info[i][1] = \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Работаем с NLP алгоритмами для данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pupki\\anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache superset -> Навык\n",
      "математический анализ -> Навык\n",
      "знания в области логистики -> Навык\n",
      "имитационное моделирование -> Навык\n",
      "power query -> Навык\n",
      "разработка проектной документации -> Навык\n"
     ]
    }
   ],
   "source": [
    "# Импортируем библиотеку для работы с нейросетями\n",
    "import torch\n",
    "# Из библиотеки импортируем модули модели BERT и токенайзера для нее\n",
    "from transformers import BertModel, BertTokenizer\n",
    "# Из библиотеки импортируем реализацию классификатора к ближайших соседей\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Загружем токенайзер для модели Bert\n",
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "# Загружаем модель типа Bert\n",
    "model = BertModel.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "\n",
    "# Создаем словарь для обучения модели\n",
    "d = {i: 'Навык' for i in all_skills} | {i: 'Не навык' for i in ['предоставляем', 'банкам', 'region', 'приглашаем', 'company', 'компания', 'маркетинговых', 'потоков', 'компанией']}\n",
    "\n",
    "# Сохраняем признаки X для обучения\n",
    "words = list(d.keys())\n",
    "\n",
    "# Создаем массив для сохранения векторных представлений слов\n",
    "word_embeddings = []\n",
    "# Циклом проходимся по всем слова\n",
    "for word in words:\n",
    "    # Токенезиурем слово и преобразуем в тензор\n",
    "    inputs = tokenizer(word, return_tensors='pt', max_length=128 ,truncation=True)\n",
    "    # Запрещаем torch использовать градиентный спуск\n",
    "    with torch.no_grad():\n",
    "        # Получаем выходной слой модели\n",
    "        outputs = model(**inputs)\n",
    "        # Усредняем по токенам\n",
    "        word_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        # Добавляем в ранее созданный массив\n",
    "        word_embeddings.append(word_embedding)\n",
    "\n",
    "# Создаем алгоритм KNN\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "# Фитим нашу модель\n",
    "knn_classifier.fit(word_embeddings, list(d.values()))\n",
    "\n",
    "# Тестово предиктим \n",
    "predicted_classes = knn_classifier.predict(word_embeddings)\n",
    "\n",
    "# Создаем счетчик\n",
    "count = 0\n",
    "# Выводим пример работы модели\n",
    "for word, predicted_class in zip(words, predicted_classes):\n",
    "    print(f'{word} -> {predicted_class}')\n",
    "    if count == 5:\n",
    "        break\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем массив со всеми описаниями\n",
    "texts = [i[1] for i in all_info]\n",
    "\n",
    "# Создаем массив для хранения в нем классов для каждого предложения\n",
    "predicted_classes_per_sentences = []\n",
    "\n",
    "# Циклом проходимся по всем данным\n",
    "for text in texts:\n",
    "    # Создаем из текста массив слов\n",
    "    words = text.split()\n",
    "    # Создаем массив для хранения в нем векторных представлений слов\n",
    "    word_embeddings = []\n",
    "    # Циклом проходимся по всем словам\n",
    "    for word in words:\n",
    "        # Токенезиурем слово и преобразуем в тензор\n",
    "        inputs = tokenizer(word, return_tensors='pt', max_length=128 ,truncation=True)\n",
    "        # Запрещаем torch использовать градиентный спуск\n",
    "        with torch.no_grad():\n",
    "            # Получаем выходной слой модели\n",
    "            outputs = model(**inputs)\n",
    "            # Усредняем по токенам\n",
    "            word_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "            # Добавляем в ранее созданный массив\n",
    "            word_embeddings.append(word_embedding)\n",
    "\n",
    "    # Предиктим классы\n",
    "    predicted_classes = knn_classifier.predict(word_embeddings)\n",
    "    # Добавляем полученные классы в ранее созданный массив\n",
    "    predicted_classes_per_sentences.append(predicted_classes)\n",
    "\n",
    "\n",
    "# Создаем массивы для всех вакансий, где будем хранить все обнаруженные навыки\n",
    "for i in range(len(all_info)):\n",
    "    all_info[i].append([])\n",
    "\n",
    "# Создаем счетчик\n",
    "count = 0\n",
    "# Прохожимся по всем данным\n",
    "for text, predicted_classes in zip(texts, predicted_classes_per_sentences):\n",
    "    # Из текста получаем массив слов\n",
    "    words = text.split()\n",
    "    # Циклом проходимся по всем словам и их классам\n",
    "    for word, predicted_class in zip(words, predicted_classes):\n",
    "        # Тестово показываем работу модели\n",
    "        if count < 10:\n",
    "            print(f'{word} -> {predicted_class}')\n",
    "        # Проверяем условие, если класс == \"Навык\" и длинна слова больше 2-х, то добавляем в массив\n",
    "        if predicted_class == 'Навык' and len(word) > 2:\n",
    "            all_info[count][4].append(word)\n",
    "\n",
    "    # Удаляем из массива дубликаты навыков\n",
    "    all_info[count][4] = list(set(all_info[count][4]))\n",
    "    # Увеличиваем счетчик\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pupki\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# Из библиотеки импортируем модуль для работы с кластерами\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Создаем массив для сохранения всех навыков в одном месте\n",
    "all_detected_skills = []\n",
    "# Циклом проходимся по всем данным\n",
    "for i in range(len(all_info)):\n",
    "    all_detected_skills += all_info[i][4]\n",
    "\n",
    "# Задаем максимальную длину последовательности\n",
    "max_length = 32\n",
    "\n",
    "# Векторизация слов\n",
    "encoded_texts = [tokenizer(word, return_tensors='pt', padding='max_length', max_length=max_length, truncation=True) for word in all_detected_skills]\n",
    "input_ids = torch.cat([encoded_text['input_ids'] for encoded_text in encoded_texts], dim=0)\n",
    "attention_masks = torch.cat([encoded_text['attention_mask'] for encoded_text in encoded_texts], dim=0)\n",
    "\n",
    "# Получаем эмбеддингов слов с помощью Bert\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_masks)\n",
    "    word_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "# Задаем количество кластеров\n",
    "num_clusters = 3\n",
    "# Создаем алгоритм KMeans\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "# Фитим нашу модель\n",
    "kmeans.fit(word_embeddings)\n",
    "\n",
    "# Создаем многомерный массив для сохранения навыков\n",
    "cluster_skills = [[] for _ in range(num_clusters)]\n",
    "\n",
    "# Заполняем созданный массив навыками\n",
    "for skill, cluster_label in zip(all_detected_skills, kmeans.labels_):\n",
    "    cluster_skills[cluster_label].append(skill)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Работаем с анализом данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### 4.1 Определяем наиболее востребованные и наименее востребованные навыки для выбранной группы вакансий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее востребованные навыки: [('знание', 67), ('sql', 59), ('быть', 57), ('график', 47), ('автоматизация', 37), ('рост', 35), ('рынок', 34), ('предлагать', 33), ('python', 30), ('excel', 28)]\n",
      "\n",
      "Наименее востребованные навыки: [('curl', 1), ('kubernetes', 1), ('финтехopen', 1), ('нефункциональный', 1), ('опцион', 1), ('dod', 1), ('бэклога', 1), ('story', 1), ('uml', 1), ('ddd', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Из библиотеки имопртируем модуль счетчика\n",
    "from collections import Counter\n",
    "\n",
    "# Считаем вхождение всех навыков \n",
    "skill_counter = Counter(all_detected_skills)\n",
    "\n",
    "# Выводим наиболее востребованные навыки\n",
    "print(f'Наиболее востребованные навыки: {skill_counter.most_common(10)}', end='\\n\\n')\n",
    "# Выводим наименее востребованные навыки\n",
    "print(f'Наименее востребованные навыки: {skill_counter.most_common()[-10:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### 4.2 Определяем наиболее оплачеваемые навыки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее оплачеваемые навыки: ['brands', 'дать', 'branch', 'mesh', 'разработкуаналитический', 'воркшопы', 'тесткейсов', 'appmetrica', 'понастоящему', 'ios', 'skyeng', 'deep', 'sql', 'analytics', 'аналитикааналитика', 'насколько', 'eltetlинструмент', 'тпбудет', 'предлагать', 'знание', 'приготовить', 'postgre', 'заниматьсяразработка', 'debezium', 'развивать', 'данныхумение', 'мэппингов', 'крутой', 'математик', 'azure', 'cicd', 'стек', 'google', 'загружать', 'счёт', 'доработка', 'superset’а', 'дмс', 'удаленныйофисныйгибридный', 'быть', 'попап', 'iceberg', 'удаленкаоформление', 'оплата', 'курс', 'mppcloud', 'selfserve', 'математический', 'senior', 'данныхпроектирование', 'redshift', 'кхд', 'ценность', 'snowflake', 'winwin', 'mlops', 'настройка', 'повышение', 'предстоять', 'delta', 'оптимизацииизменении', 'тестированиеответственность', 'binom', 'пиццамейкерам', 'реклама', 'писать', 'dbt', 'твой', 'kafka', 'тулинга', 'перфекционизмом', 'маппингов', 'страныоформление', 'уметь', 'пайплайнов', 'java', 'разработкудоработку', 'заниматьсяучастие', 'проработка', 'работынастройка', 'функциональный', 'ads', 'github', 'appsflyer', 'analyst', 'становиться', 'greenplam', 'dodo', 'datadriven', 'oracleпроработка', 'apache', 'детальный', 'python', 'alter', 'databricks', 'dashboarding', 'коннектор', 'superset', 'bigquery', 'meta', 'greenplum', 'quality', 'rabbitmq', 'кто', 'explorer', 'vertica', 'чужой', 'etlпроцессовразработка', 'sqlкодзнание', 'airflow', 'временивозможность', 'плаз', 'источниковнаши', 'etl', 'хороший', 'ожидаемопыт', 'москва', 'teradata', 'децентрализованный', 'митапы', 'data', 'тебе', 'dwh', 'банкот', 'данныхопыт', 'learning', 'формировать', 'biсистемы', 'mlflow', 'production', 'pizza', 'mlмоделей', 'творить', 'функция', 'иили', 'дашбордов', 'api', 'actions', 'компанииприсоединяйтесь', 'deltalake', 'clickhouse', 'looker', 'отчетностиконтроль', 'визуализация', 'lake', 'ипполная', 'мышление', 'отчетовпроработка', 'балансировать', 'витриннаписание', 'так', 'мэппинги', 'хранилищаопыт', 'kebster', 'тебя', 'tableau', 'eventhubs', 'tiktok', 'warehouse', 'уровнеусловиялокация', 'streaming', 'рфполная', 'автоматизация', 'аналитикахорошее', 'данныхинтеграция', 'реляционный', 'ожидание', 'adjust', 'построение', 'mvp', 'lineage', 'hudi', 'оптимальный', 'structured', 'привет', 'рекламный', 'drinkit', 'engineerчем', 'scala', 'batch', 'платформенный', 'логированием', 'имусловияудаленный']\n"
     ]
    }
   ],
   "source": [
    "# Создаем массив, куда сохраним наиболее оплачиваемые навыки\n",
    "most_paid_skills = []\n",
    "# Находим среднюю зарплату\n",
    "avg_salary = sum([i[2] for i in all_info]) / len([i[2] for i in all_info])\n",
    "# Проходимся по всем данным\n",
    "for i in range(len(all_info)):\n",
    "    # Проверяем условие, если зарплата вакансии в два раза выше, чем средняя зарплата, то добавляем в массив\n",
    "    if all_info[i][2] > avg_salary * 2:\n",
    "        most_paid_skills += all_info[i][4]\n",
    "\n",
    "# Убираем дубликаты навыков\n",
    "most_paid_skills = list(set(most_paid_skills))\n",
    "# Выводим результат\n",
    "print(f'Наиболее оплачеваемые навыки: {most_paid_skills}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### 4.3 Орпеделяем региональную специфику востребованных навыков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Создаем словарь для наиболее востребованных навыков\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m most_needed_skills \u001b[38;5;241m=\u001b[39m {name: \u001b[38;5;28msum\u001b[39m([i[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m all_info \u001b[38;5;28;01mif\u001b[39;00m i[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m name], []) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [i[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_info\u001b[49m]}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Проходимся циклом по словарю\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m region, skills \u001b[38;5;129;01min\u001b[39;00m most_needed_skills\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Считаем вхождение навыков для региона\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_info' is not defined"
     ]
    }
   ],
   "source": [
    "# Создаем словарь для наиболее востребованных навыков\n",
    "most_needed_skills = {name: sum([i[4] for i in all_info if i[3] == name], []) for name in [i[3] for i in all_info]}\n",
    "# Проходимся циклом по словарю\n",
    "for region, skills in most_needed_skills.items():\n",
    "    # Считаем вхождение навыков для региона\n",
    "    skill_counter = Counter(skills)\n",
    "    # Добавляем 5 наиболее востребованных навыков для региона\n",
    "    most_needed_skills[region] = skill_counter.most_common(5)\n",
    "\n",
    "# Выводим наиболее востребованные навыки для каждого региона\n",
    "for region in most_needed_skills.keys():\n",
    "    print(f'{region.capitalize()}: {most_needed_skills[region]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Работаем с сохранением данных в Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем билиотеку для работы с Excel на питоне\n",
    "import openpyxl\n",
    "\n",
    "# Создаем массив для хранения данных\n",
    "data_for_excel = [[] for _ in range(num_clusters)]\n",
    "# Проходимся по всем кластерам\n",
    "for i in range(num_clusters):\n",
    "    # Считаем вхождение навыков в каждом кластере\n",
    "    skill_counter = Counter(cluster_skills[i])\n",
    "    # Сохраняем в ранее созданный массив\n",
    "    data_for_excel[i] = skill_counter\n",
    "\n",
    "# Создаем лист Excel\n",
    "wb = openpyxl.Workbook()\n",
    "# Получаем активный лист\n",
    "sheet = wb.active\n",
    "\n",
    "# Создаем заголовки для листа\n",
    "headers = ['Навык', 'Количество', 'Кластер']\n",
    "# Циклом заполняем заголовки на первой строке\n",
    "for col, header in enumerate(headers, start=1):\n",
    "    # Внутренним методом заполняем первую строку\n",
    "    sheet.cell(row=1, column=col, value=header)\n",
    "\n",
    "# Задаем индекс строки\n",
    "row_index = 2\n",
    "\n",
    "# Циклом проходимся по всем кластерам\n",
    "for cluster_index, cluster in enumerate(data_for_excel, start=1):\n",
    "    # Циклом заполняем строки\n",
    "    for skill, count in cluster.items():\n",
    "        sheet.cell(row=row_index, column=1, value=skill)\n",
    "        sheet.cell(row=row_index, column=2, value=count)\n",
    "        sheet.cell(row=row_index, column=3, value=f'Кластер {cluster_index}')\n",
    "        # Увеличиваем индекс строки\n",
    "        row_index += 1\n",
    "\n",
    "# Сохраняем лист Excel\n",
    "wb.save('mainskills.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Сохраняем данные с этого модуля, для заполнения графа связей на следующем модуле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем данные в массив\n",
    "data = [i[4] for i in all_info]\n",
    "\n",
    "# Открываем файл\n",
    "with open('dataModuleB.txt', 'w') as f:\n",
    "    # Циклом проходимся по данным\n",
    "    for value in data:\n",
    "        # Сохраняем данные в файл\n",
    "        f.write(f'{value},\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
